<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>octave on James Horton</title>
        <link>https://jjhorton.co.uk/tags/octave/</link>
        <description>Recent content in octave on James Horton</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-uk</language>
        <lastBuildDate>Thu, 10 Mar 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://jjhorton.co.uk/tags/octave/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Octave on my Raspberry Pi Cluster</title>
        <link>https://jjhorton.co.uk/posts/octave-on-my-raspberry-pi-cluster/</link>
        <pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate>
        
        <guid>https://jjhorton.co.uk/posts/octave-on-my-raspberry-pi-cluster/</guid>
        <description>&lt;img src="https://jjhorton.co.uk/img/picluster.jpg" alt="Featured image of post Octave on my Raspberry Pi Cluster" /&gt;&lt;p&gt;The main aim of building my Raspberry Pi Cluster was so that I could run
octave code on it, now I&amp;rsquo;m not expecting to break any records, but
ideally I want some performance. The best way I found to do this was
with the parallel&amp;rsquo;s package.&lt;/p&gt;
&lt;p&gt;With octave and the parallels package installed on each of the Raspberry
Pi 2&amp;rsquo;s in the cluster, I setup SSH so that no passwords are required
between the nodes. The first step in running the octave scripts was to
launch the parallels server on each of theÂ  3 other Raspberry Pi&amp;rsquo;s that
I&amp;rsquo;m not connected to&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Bash&#34; data-lang=&#34;Bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;\#&lt;/span&gt;!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;USERNAME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;pi
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;HOSTS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;rpi1 rpi2 rpi3&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;SCRIPT&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;cd Documents/Octave; octave -q clusterStart.m&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; HOSTNAME in &lt;span class=&#34;se&#34;&gt;\$&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;HOSTS&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ssh -l &lt;span class=&#34;se&#34;&gt;\$&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;USERNAME&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\$&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;HOSTNAME&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;\${SCRIPT}&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This launches Octave and runs the following script:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Bash&#34; data-lang=&#34;Bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pkg load parallel
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;addpath&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;\~/Documents/Octave/&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;addpath&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;\~/Documents/Octave/Server&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pserver
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With each of these raspberry pi&amp;rsquo;s now running on the 3 other Pi&amp;rsquo;s it is now possible to send simple commands to them and get the results returned to the first Pi. The octave script that I used to test this out is bellow&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Bash&#34; data-lang=&#34;Bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;clear&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pkg load parallel
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;hosts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;rpi0&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;rpi1&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;rpi2&amp;#39;&lt;/span&gt;, &lt;span class=&#34;s1&#34;&gt;&amp;#39;rpi3&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;sockets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; connect&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;hosts&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;psum&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; zeros&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;1,3&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;reval&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;send(sum(\[1:10\]),sockets(1,:))&amp;#34;&lt;/span&gt;, sockets&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;2,:&lt;span class=&#34;o&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;reval&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;send(sum(\[11:20\]),sockets(1,:))&amp;#34;&lt;/span&gt;, sockets&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;3,:&lt;span class=&#34;o&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;reval&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;send(sum(\[21:30\]),sockets(1,:))&amp;#34;&lt;/span&gt;,sockets&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;4,:&lt;span class=&#34;o&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;psum&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;1&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; recv&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;sockets&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;2,:&lt;span class=&#34;o&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;psum&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;2&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; recv&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;sockets&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;3,:&lt;span class=&#34;o&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;psum&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;3&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; recv&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;sockets&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;4,:&lt;span class=&#34;o&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sum&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;psum&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;scloseall&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;sockets&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This sets up the sockets for all 4 Raspberry Pi&amp;rsquo;s including the one the code is run on (rpi0) and the 3 that are running the pserver (rpi1, rpi2 and rpi3). The code then sends commands as strings to be evaluated at the pserver&amp;rsquo;s, which in this case are simple summation&amp;rsquo;s, and then return the values to rpi0, which adds the 3 values they generated together and displays it on the screen.&lt;/p&gt;
&lt;p&gt;The next step for me in this project is to get it running my Twitter Sentiment Code.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Over clocking my Raspberry Pi cluster</title>
        <link>https://jjhorton.co.uk/posts/over-clocking-my-raspberry-pi-cluster/</link>
        <pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate>
        
        <guid>https://jjhorton.co.uk/posts/over-clocking-my-raspberry-pi-cluster/</guid>
        <description>&lt;img src="https://jjhorton.co.uk/img/picluster.jpg" alt="Featured image of post Over clocking my Raspberry Pi cluster" /&gt;&lt;p&gt;With my newly created raspberry pi cluster being made out of the
Raspberry Pi 2, I don&amp;rsquo;t want it to lag to far behind the freshly
announced Raspberry Pi 3. To try and keep up with the new hardware
without replacing all of my Pi&amp;rsquo;s I devices it was worth a go at
overclocking them.&lt;/p&gt;
&lt;p&gt;Overlooking my Pi&amp;rsquo;s should allow them to work a bit quicker, but it will reduce power efficient and increase the tempurature that the cluster runs at. Overclocking can be a dangerous game, as if you run the Pi&amp;rsquo;s to hard you could easily cause hardware damage to the boards if the tempurature gets to high. Thankfully there are plenty of &lt;a class=&#34;link&#34; href=&#34;http://haydenjames.io/raspberry-pi-2-overclock/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;guides on the
Internet&lt;/a&gt; to talk you through sure tasks which is what I used.&lt;/p&gt;
&lt;p&gt;All you have to do is edit the&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo nano /boot/config.txt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I then added the following at the end of the file, which increase not
only the clocking frequency, but also the sdram and core frequency. It
also enables over voltage and sets an upper temperature limit.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;arm_freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;sdram_freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;core_freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;over_voltage&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;temp_limit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;80&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#Will throttle to default clock speed if hit&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After which all there is to do is reboot the Raspberry Pi with&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo reboot
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now time to test some code of my cluster and see what the performance increase is&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Building a Raspberry Pi Cluster</title>
        <link>https://jjhorton.co.uk/posts/building-a-raspberry-pi-cluster/</link>
        <pubDate>Wed, 02 Mar 2016 00:00:00 +0000</pubDate>
        
        <guid>https://jjhorton.co.uk/posts/building-a-raspberry-pi-cluster/</guid>
        <description>&lt;img src="https://jjhorton.co.uk/img/picluster.jpg" alt="Featured image of post Building a Raspberry Pi Cluster" /&gt;&lt;p&gt;I&amp;rsquo;ve had a long standing interest in Parallel Processing, and then a few months ago I came across Octave and its parallel package. Octave is very similar to MATLAB that I use every day, and for the most part basic MATLAB code works out of the box in Octave.&lt;/p&gt;
&lt;p&gt;I used this as the perfect excuse to build myself a mini-cluster computer&amp;hellip;&lt;/p&gt;
&lt;p&gt;The mini cluster needed to be cheep, so I based it on the raspberry pi. After reading around on the internet it became apparent that this has been done before, so I decided that I needed to give it a go and build my own clusterÂ and run someÂ octave code.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jjhorton.co.uk/img/IMG_1223.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Here is the result of my work, all held together with velcro with the operating system installed on each unit. I based the setup on &lt;a class=&#34;link&#34; href=&#34;http://makezine.com/projects/build-a-compact-4-node-raspberry-pi-cluster/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;this&lt;/a&gt; tutorial, but instead of using a Usb Ethernet adapter on the controller I used a WIFI adapter so it could be placed anywhere around the house.&lt;/p&gt;
&lt;p&gt;Things I need to do next are, installing and setting up Octave, also looking to see if I can make a cable to power the network switch from the spare port USB hub, so that the cluster can run of a single plug socket.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
